# Reasoning360

The repo is an attempt to replicate the large-scale RL training of [DeepSeek R1](https://github.com/deepseek-ai/DeepSeek-R1).

It's initialized from [verl](https://github.com/volcengine/verl). verl's README is appended below.

## Setup
Check the guide of verl to setup the environment.

Remember to process data and wandb login before launching the experiments.

---
## How to add the dataset

### Preprocessing

In preprocessing, we usually load a raw dataset from huggingface, process it into a list of dictionaries, and then save it into a parquet file.


1. Prompt preprocessing

    We need to process the raw question into a prompt ready to be fed to the LLM.

    Depending on the specific LLM to be used, there are two kind of prompts

    - Chat style

        This is used to train a model that has been trained on long-CoT, including `LLM360/Reason-SFT-32B-v0.1`, `deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B`.

        In the dict, we need to specify the prompt within the data dict:
        ```             
        "prompt": [{
            "role": "user",
            "content": question
        }],
        ```

        When we use verl to train the model, it will turn into a prompt string with `apply_chat_template`.

        Note that:
        - You will need to add some task-specific instruction in the `question`. E.g., for math, we concatenate the raw problem with `Let's think step by step and output the final answer within \\boxed{}.`, so that it's easy to extract the answer from model output.
        - These models are not trained with system prompt, so there is no need to add system prompt.
        - The `apply_chat_template` method of these two models will automatically append `<think>` to the end of the prompt


    - R1-zero style

        This is used for training from a base model (without instruction-tuned), e.g., `Qwen/Qwen2.5-32B`.

        In this case, since the model is not trained to use `<think>`, and it'll not automatically add `<think>` in `apply_chat_template`, we will need to do this processing here ourselves.

        First define a template:
        ```
        template = """A conversation between User and Assistant. The user asks a question, and the Assistant solves it. The assistant first thinks about the reasoning process in the mind and then provides the user with the response. The reasoning process is enclosed within <think> </think> i.e., <think> reasoning process here </think> respond to the user's question here.
        User: {{prompt}} Please put your answer in \\boxed{} tags.
        Assistant: <think>"""
        ```
        Then, in the data dict, we need to specify:
        ```
        "raw_prompt": template.replace("{{prompt}}", question),
        "apply_chat_template": False,
        ```
        With these two keys, verl will not apply chat template to the `prompt`, but will directly use `raw_prompt`.

    2. Reward function

    We need to specify the 

### 1. Math
Take `Deepscaler` as an example.
Deepscaler has 40K high-quality math (Q, A) pairs from previous AIME, AMC, etc. Prepare this by running:
```bash
python examples/data_preprocess/deepscaler_preview.py --local_dir data/deepscaler_preview
```

### 2. Code
Take `Kodcode` as an example.
Kodcode is a dataset of 400K code (Q, solution, utests) triplets from previous ACM ICPC, etc. Prepare this by running:
```bash
python examples/data_preprocess/codgen.py --dataset_names kodcode
```

### 3. Logic
Take `Ordering Puzzle` as an example.
```bash
python examples/data_processs/puzzle_gen.py --num_puzzles 10000 --output_dir data/puzzles_dataset --output_file puzzles_dataset.json --test True
```


---
## Training (docker + fsdp/megatron)
Multi-node training 7B w/ FSDP backend:
```bash
sbatch scripts/templates/docker-fsdp-qwen7b-4nodes-grpo.sh
```

Multi-node training 32B w/ FSDP backend:

(Note: as vllm updates to >= 0.8, the docker image needs update; can turn to local conda env for now)
```bash
sbatch scripts/templates/docker-fsdp-qwen32b-16nodes-grpo.sh
```

Multi-node training 32B w/ Megatron backend:
```bash
sbatch scripts/templates/docker-megatron-dsr1_qwen_32b-16nodes-ppo.sh
```

## Usage (conda+fsdp)
First export the conda binary path (can check with `which python` and `which ray`):
```
export CONDA_BIN_PATH=/path/to/conda/bin/
```

Single node, 8-H100 GPUs:
```bash
bash scripts/templates/conda-fsdp-qwen2.5_math_3b-1node-ppo.sh
```

Multi-node training 7B:
```bash
sbatch scripts/templates/conda-fsdp-qwen7b-4nodes-grpo.sh
```

Multi-node training 32B:
```bash
sbatch scripts/templates/conda-fsdp-qwen32b-16nodes-grpo.sh
```

The single-node script directly prints log in the terminal; the multi-node script prints log in the slurm log (`.out` and `.err`) files. Check wandb for the experiment logs.

Adjust the template script to fit your needs.

## Change reward metric
Set `reward_model.reward_metric` in the config file or cli arguments. Can choose from "prime_math", "math_verify", "boxed_math_verify" for math. Default is None, i.e., use "prime_math" for math.


### To add new dataset...

Just add the dataset to the `examples/data_preprocess/` and follow the format of the existing datasets.

---
## Data viewer

1. Installation

First install streamlit with
```bash
pip install streamlit
```

2. Running

Then run 
```bash
streamlit run data_viewer.py
```

For custom data sources, you can specify arguments:
```bash
streamlit run data_viewer.py -- --source-type wandb_table --file-path path/to/your/file.json
```

Currently it only supports loading the wandb logging table (enabled with `trainer.val_generations_to_log_to_wandb=...` in a training script).

We are extending it to other data sources.

3. Viewing the data

Need to set up port forwarding
```bash
ssh -L 8501:localhost:8501 username@cluster_node
```

Then access `127.0.0.1:8501`


## Data analysis

Split a dataset, and generate multiple responses with SGLang.

```python transform_dataset.py --dataset SynthLabsAI/Big-Math-RL-Verified --output big-math-rl-verified_problems --splits 64```

install SGLang from github (need to be after this [PR](https://github.com/sgl-project/sglang/pull/3754) to avoid a bug in batch inference)

```bash data_analysis/sgl_generate.sh```

The script will automatically launch 64 jobs (each on one node) and monitor them. If some job fails unexpectedly, you can launch the job in `temp_job_scripts` and add the new job id to `sgl_job_ids.txt` so that it'll be monitored.


## Contributing

We use `yapf` to enforce strict code formatting. Before committing, make sure you have run the pre-commit checks.
```bash
pre-commit install
pre-commit run --all-files
```

---

## Trouble shooting
1. Gloo socket issue:
```bash
export GLOO_SOCKET_IFNAME=ens10f0np0
```
already added in the template scripts.

2. Training got stuck after several steps rollout phase.
We found it happens in `vllm` all-reduce operations. Setting the following
```
    actor_rollout_ref.rollout.enforce_eager=True \
    actor_rollout_ref.rollout.free_cache_engine=True \
``` 
in the script would solve it.
